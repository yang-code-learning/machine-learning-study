# 监督学习

### 1 分类与回归

- 监督学习问题的主要两种：分类与回归
- **分类（离散）**
  - 目标：预测类别标签（针对离散数据）
  - 二分类问题 & 多分类问题
    - 正类和反类
- **回归（连续）**
  - 目标：预测一个连续值（浮点数 实数）



### 2 泛化、过拟合和欠拟合

- 泛化
  - 对没见过的数据做出精确预测：称从训练集泛化到测试集
  - 构建一个泛化精度尽可能高的模型
- 过拟合
  - 过分关注训练集的细节
  - 训练集很好，但不能泛化到新数据
- 欠拟合
  - 无法抓住数据的重点和变化
  - 训练集和测试集都不行



### 3 监督学习

#### 3.1 一些样本数据集

- forge 数据集：两个特征，用于分类，没有实际意义
- wave 数据集：一个输入特征和响应，用于回归，没有实际意义
- 威斯康辛州乳腺癌数据集
  - 标记为 良性 或 恶性 的肿瘤
  - 569 个数据点和 30 个特征
  - 数据集保存为Bunch类型，可用点操作符访问或字典的用法
- 波士顿房价数据集
  - 506 个数据点和 13 个特征

#### 3.2 K 近邻

- k 近邻分类
  - **算法：考虑最近的k个邻居，根据他们的标签配以距离度量方法得出预测结果**
- 分析 KNeighborsClassifier
  - 邻居过多往往训练精度下降
  - 对于测试集，过多或过少精度都不高
- k 近邻回归
- 优点、缺点和参数
  - 两个重要参数：**邻居个数** 和 **数据点之间距离的度量方法**
  - 易于理解 但是 不实用
  - 不适用 稀疏数据集 和 多特征数据集

#### 3.3 线性模型

- 用于回归的线性模型
  - 利用输入的线性函数进行预测
  - 对单一特征是一条直线，对两个特征是一个平面，更高维则是超平面
- 线性回归（普通最小二乘法）
  - **算法：寻找参数 w 和 b 使得训练集的预测值和真实的回归目标值 y 之间的均方误差最小**
  - **线性回归没有参数**，也因此没法控制复杂度
- 岭回归
  - 算法：在线性回归基础上对权值做约束，即正则化（系数尽可能小以减少对特征的影响，但又能保证能够拟合目标），目标是避免过拟合
  - 岭回归采用 L2 正则化（欧式距离）
  - **参数是alpha**，增大alpha会增大对系数抑制，越趋近于0
  - 值得注意的是，线性回归在数据集大的情况下将和岭回归拥有相似性能，即**如果数据集足够大，正则化变得不那么重要**；还有数据增大，训练集精度下降，模型难以记住每个数据了
- Lasso
  - Lasso 回归采用 L1 正则化（系数的绝对值之和）
  - 效果是权值要么为零，要么非零；某些特征会被完全忽略
  - 这样解释性更好，仅展示最重要的几个特征
- 用于分类的线性模型
  - 常见的：**Logistic 回归** 和 **线性支持向量机**
  - 参数有**正则化强度**C，越大正则化越弱（想正则化就往小调）
  - 低维空间首先，高维下线性分类非常强大
  - **参数除了正则化强大C，还有 penalty，可设置采用 L1 还是 L2**
- 用于多分类的线性模型
  - 算法：”一对其余“，对每个类别都学习一个二分类模型，预测时在对应类别模型得分最高的分类器获胜

- 优点、缺点和参数
  - 数据量大，线性回归问题可能需要设置 slover="sag" 选项
  - 最重要的参数就是正则化，回归模型叫alpha，分类模型叫做C
  - **如果特征数大于样本数，线性模型的表现通常非常好**


#### 3.4 朴素贝叶斯分类器

- 和线性模型非常相似、高效率但是泛化能力低
- sklearn 中实现了三种
  - GuassianNB：可应用任何连续数据
  - BernoulliNB：假定输入数据为二分类数据
  - MultionmialNB：假定输入数据为计数数据

- 优点、缺点和参数
  - 都只有一个参数alpha，控制模型复杂度，alpha越大，平滑化越强，模型复杂度越弱
  - alpha对模型性能不重要，但是通常会使精度略有提高
  - GuassianNB主要用于高维数据，其他广泛用于稀疏数据。

#### 3.5 决策树

- 构造决策树
  - 测试：“特征 i 的值是否大于 a ”
  - **算法：对于所有测试，找出对目标变量来说信息量最大的那一个**
- 控制决策树的复杂度
  - 预剪枝 和 后剪枝（sklearn 上仅实现了预剪枝）
  - 参数就是 depth，控制树的深度（达到深度就停止延展）
- 分析决策树
- 树的特征重要性
  - 重要度很小并不意味着不重要，可能特征没有被选中，该特征实际上可由另一个衍生
  - 特殊性质：不能外推，不能再训练数据范围之外进行预测
- 优点、缺点和参数
  - 参数就是预剪枝参数：max_depth, max_leaf_nodes, min_samples_leaf
  - 模型可视化容易；不受数据缩放的影响
  - 即使做了预剪枝，还是可能过拟合

#### 3.6 决策树集成

- 随机森林
- 梯度提升回归树（梯度提升机）


​	
